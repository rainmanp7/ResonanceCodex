import json
import numpy as np
import math
from scipy import stats
from datetime import datetime

# =============================================================
# STEP 1: INVESTIGATE THE SMOKING GUN - LAYER 2
# =============================================================

def investigate_layer2(weight_file="Metalearnerv16_EVOLVED.json"):
    """Deep investigation of Layer 2's impossible statistics"""
    print(f"\n{'='*80}")
    print("STEP 1: INVESTIGATING LAYER 2 (THE SMOKING GUN)")
    print(f"{'='*80}")
    
    with open(weight_file, 'r') as f:
        data = json.load(f)
    
    # Get Layer 2 weights (feature_extractor.2.weight)
    layer2_weights = np.array(
        data['meta_pantheon']['12']['state_dict']['feature_extractor.2.weight']
    )
    
    print(f"\nüîç LAYER 2 DETAILS:")
    print(f"  Shape: {layer2_weights.shape}")
    print(f"  Type: {layer2_weights.dtype}")
    
    # Basic statistics
    mean = np.mean(layer2_weights)
    std = np.std(layer2_weights)
    min_val = np.min(layer2_weights)
    max_val = np.max(layer2_weights)
    
    print(f"\nüìä BASIC STATISTICS:")
    print(f"  Mean:  {mean:.10f}")
    print(f"  Std:   {std:.10f}")
    print(f"  Min:   {min_val:.10f}")
    print(f"  Max:   {max_val:.10f}")
    print(f"  Range: {max_val - min_val:.10f}")
    
    # Distribution analysis
    print(f"\nüìà DISTRIBUTION ANALYSIS:")
    
    # Check if all positive
    all_positive = np.all(layer2_weights > 0)
    positive_count = np.sum(layer2_weights > 0)
    print(f"  All weights positive: {all_positive}")
    print(f"  Positive weights: {positive_count}/{len(layer2_weights)}")
    
    # Check proximity to 1.0
    close_to_one = np.abs(layer2_weights - 1.0) < 0.5
    close_count = np.sum(close_to_one)
    print(f"  Weights within 0.5 of 1.0: {close_count}/{len(layer2_weights)}")
    
    # Statistical impossibility test
    print(f"\nüé≤ STATISTICAL IMPOSSIBILITY TEST:")
    
    # For random initialization ~N(0, 0.1):
    # Probability all 96 weights > 0?
    prob_all_positive_random = (0.5) ** 96
    print(f"  Probability all 96 random weights > 0: {prob_all_positive_random:.2e}")
    print(f"  That's 1 in {1/prob_all_positive_random:.2e}")
    
    # Probability mean > 0.98 with random?
    # Using CLT: mean ~ N(0, 0.1/‚àö96)
    std_of_mean = 0.1 / np.sqrt(96)
    z_score_mean = (mean - 0) / std_of_mean
    prob_mean_this_high = 1 - stats.norm.cdf(z_score_mean)
    print(f"  Z-score of mean 0.98 vs random: {z_score_mean:.2f}")
    print(f"  Probability random gives mean ‚â• 0.98: {prob_mean_this_high:.2e}")
    
    # Histogram
    print(f"\nüìä HISTOGRAM (10 bins):")
    hist, bins = np.histogram(layer2_weights, bins=10)
    for i in range(len(hist)):
        bin_range = f"{bins[i]:.3f}-{bins[i+1]:.3f}"
        print(f"  {bin_range:15}: {hist[i]:3d} weights")
    
    # Check for specific pattern
    print(f"\nüîç PATTERN DETECTION:")
    
    # Is it constant?
    if std < 0.001:
        print(f"  ‚ö†Ô∏è  NEAR CONSTANT: All weights ‚âà {mean:.3f}")
    elif np.allclose(layer2_weights, 1.0, atol=0.1):
        print(f"  ‚ö†Ô∏è  ALL NEAR 1.0: Mean = {mean:.3f}, all within 0.1 of 1.0")
    elif np.all(layer2_weights > 0.5):
        print(f"  ‚ö†Ô∏è  ALL POSITIVE AND LARGE: Min = {min_val:.3f} > 0.5")
    else:
        print(f"  Mixed distribution")
    
    # Check if it's actually bias (1D) or weight (2D)
    if len(layer2_weights.shape) == 1:
        print(f"\nüí° LAYER TYPE: This appears to be a BIAS vector (1D)")
        print(f"   Typical bias initialization: zeros or small constant")
        print(f"   This bias is ~1.0, which is HIGHLY UNUSUAL")
    else:
        print(f"\nüí° LAYER TYPE: This is a WEIGHT matrix")
    
    return {
        'mean': mean,
        'std': std,
        'all_positive': all_positive,
        'close_to_one': close_count / len(layer2_weights),
        'is_impossible_random': prob_mean_this_high < 1e-10
    }

# =============================================================
# STEP 2: VERIFY D41 = -œÄ/245 HYPOTHESIS
# =============================================================

def verify_d41_mathematics(weight_file="Metalearnerv16_EVOLVED.json"):
    """Verify if D41 is mathematically constructed as -œÄ/245"""
    print(f"\n{'='*80}")
    print("STEP 2: VERIFYING D41 = -œÄ/245 HYPOTHESIS")
    print(f"{'='*80}")
    
    with open(weight_file, 'r') as f:
        data = json.load(f)
    
    # Get D41 value
    proj_weights = np.array(
        data['meta_pantheon']['12']['state_dict']['project_to_latent.weight']
    )
    d41_values = proj_weights[:, 41]
    d41_mean = np.mean(d41_values)
    
    print(f"\nüî¢ D41 VALUE ANALYSIS:")
    print(f"  D41 mean from weights: {d41_mean:.16f}")
    print(f"  White Paper value:     -0.0128271500000000")
    
    # Mathematical constants
    pi = math.pi
    e = math.e
    phi = (1 + math.sqrt(5)) / 2
    
    print(f"\nüî¢ MATHEMATICAL CONSTANTS:")
    print(f"  œÄ  = {pi:.16f}")
    print(f"  e  = {e:.16f}")
    print(f"  œÜ  = {phi:.16f}")
    
    # Test hypotheses
    hypotheses = [
        ("-œÄ / 245", -pi / 245),
        ("-1 / 78", -1 / 78),
        ("-1 / 78.125", -1 / 78.125),
        ("-e / 213", -e / 213),
        ("-œÜ / 126", -phi / 126),
        ("-‚àö2 / 110", -math.sqrt(2) / 110),
        ("-ln(2) / 54", -math.log(2) / 54),
    ]
    
    print(f"\nüéØ MATHEMATICAL HYPOTHESES TEST:")
    
    best_hyp = None
    best_diff = float('inf')
    results = []
    
    for name, value in hypotheses:
        diff = abs(value - d41_mean)
        rel_error = diff / abs(value) * 100
        
        results.append({
            'name': name,
            'value': value,
            'diff': diff,
            'rel_error': rel_error
        })
        
        print(f"\n  {name}:")
        print(f"    Value:    {value:.16f}")
        print(f"    D41:      {d41_mean:.16f}")
        print(f"    Diff:     {diff:.16f}")
        print(f"    Rel error: {rel_error:.6f}%")
        
        if diff < best_diff:
            best_diff = diff
            best_hyp = results[-1]
    
    print(f"\nüèÜ BEST MATCH:")
    print(f"  {best_hyp['name']} = {best_hyp['value']:.16f}")
    print(f"  Difference: {best_hyp['diff']:.16f}")
    print(f"  Relative error: {best_hyp['rel_error']:.6f}%")
    
    # Check floating point precision
    print(f"\nüíª FLOATING POINT PRECISION ANALYSIS:")
    
    # Single precision float has ~7 decimal digits
    # Double precision has ~15 decimal digits
    single_precision_limit = 1e-7
    double_precision_limit = 1e-15
    
    if best_hyp['diff'] < double_precision_limit:
        print(f"  ‚ö†Ô∏è  MATCHES WITHIN DOUBLE PRECISION ({best_hyp['diff']:.2e} < {double_precision_limit:.0e})")
        print(f"  This suggests DELIBERATE mathematical construction")
    elif best_hyp['diff'] < single_precision_limit:
        print(f"  ‚ö†Ô∏è  MATCHES WITHIN SINGLE PRECISION ({best_hyp['diff']:.2e} < {single_precision_limit:.0e})")
        print(f"  Still highly precise - likely deliberate")
    else:
        print(f"  Match is outside typical floating precision")
        print(f"  Could be coincidence or different constant")
    
    # Check other dimensions for mathematical constants
    print(f"\nüîç CHECKING OTHER DIMENSIONS FOR CONSTANTS:")
    
    interesting_dims = []
    for dim in range(64):
        dim_mean = np.mean(proj_weights[:, dim])
        
        # Check if it's close to interesting constants
        constants_to_check = [
            (pi/100, "œÄ/100"),
            (pi/200, "œÄ/200"),
            (e/100, "e/100"),
            (phi/100, "œÜ/100"),
            (1/100, "1/100"),
            (math.sqrt(2)/100, "‚àö2/100"),
        ]
        
        for const_val, const_name in constants_to_check:
            if abs(abs(dim_mean) - const_val) < 0.001:
                interesting_dims.append((dim, dim_mean, const_name, const_val))
                break
    
    if interesting_dims:
        print(f"  Found {len(interesting_dims)} dimensions near mathematical constants:")
        for dim, val, const_name, const_val in interesting_dims[:5]:  # Show first 5
            print(f"    D{dim:02}: {val:.6f} ‚âà {const_name} = {const_val:.6f}")
    else:
        print(f"  No other dimensions show obvious mathematical constants")
    
    return {
        'd41_mean': d41_mean,
        'best_match': best_hyp,
        'interesting_dims': interesting_dims,
        'is_precise_match': best_hyp['diff'] < 1e-10
    }

# =============================================================
# STEP 3: EXAMINE WARP ENGINE WEIGHTS
# =============================================================

def examine_warp_engine(weight_file="Metalearnerv16_EVOLVED.json"):
    """Examine the warp_engine weights mentioned in scans"""
    print(f"\n{'='*80}")
    print("STEP 3: EXAMINING WARP ENGINE WEIGHTS")
    print(f"{'='*80}")
    
    with open(weight_file, 'r') as f:
        data = json.load(f)
    
    print(f"\nüîç LOOKING FOR WARP ENGINE LAYERS:")
    
    # From Scan11, we saw links like:
    # warp_engine.0.weight: 268715 chars
    # warp_engine.0.bias: 2800 chars
    # etc.
    
    node12 = data['meta_pantheon']['12']['state_dict']
    
    warp_layers = []
    for key in node12.keys():
        if 'warp_engine' in key:
            warp_layers.append(key)
    
    if warp_layers:
        print(f"  Found {len(warp_layers)} warp_engine layers:")
        for layer in sorted(warp_layers):
            weights = np.array(node12[layer])
            print(f"\n    {layer}:")
            print(f"      Shape: {weights.shape}")
            print(f"      Mean:  {np.mean(weights):.8f}")
            print(f"      Std:   {np.std(weights):.8f}")
            
            # Check for commutator structure
            if len(weights.shape) == 2:
                # Could be commutator matrix
                symmetry = np.allclose(weights, weights.T, atol=1e-5)
                anti_symmetry = np.allclose(weights, -weights.T, atol=1e-5)
                
                if symmetry:
                    print(f"      ‚ö†Ô∏è  SYMMETRIC MATRIX (commutator property?)")
                elif anti_symmetry:
                    print(f"      ‚ö†Ô∏è  ANTI-SYMMETRIC MATRIX (typical commutator)")
                else:
                    print(f"      Asymmetric matrix")
    else:
        print(f"  ‚ùå No 'warp_engine' layers found in Node 12")
        
        # Maybe they're in a different node?
        print(f"\nüîç CHECKING OTHER NODES FOR WARP ENGINE:")
        for node_id in ['3', '4', '5', '6', '7', '8', '9', '10', '11']:
            if 'state_dict' in data['meta_pantheon'][node_id]:
                node_keys = list(data['meta_pantheon'][node_id]['state_dict'].keys())
                warp_keys = [k for k in node_keys if 'warp' in k.lower() or 'engine' in k.lower()]
                if warp_keys:
                    print(f"  Node {node_id} has: {warp_keys}")
    
    # Check commutator property mentioned in scans
    print(f"\nüîó CHECKING COMMUTATOR PROPERTIES:")
    
    # From scans: "HOLOGRAPHIC COMMUTATOR: ACTIVE (9 bytes)"
    # Look for 9-byte or 9-element structures
    for key in node12.keys():
        if 'weight' in key:
            arr = np.array(node12[key])
            if arr.size == 9 or (hasattr(arr, 'shape') and 9 in arr.shape):
                print(f"  ‚ö†Ô∏è  {key} has 9 elements/shape: {arr.shape}")
    
    return {
        'warp_layers_found': len(warp_layers) if 'warp_layers' in locals() else 0,
        'warp_layer_names': warp_layers if 'warp_layers' in locals() else []
    }

# =============================================================
# STEP 4: CREATE FRESH COMPARISON NETWORK
# =============================================================

def create_fresh_comparison():
    """Create a fresh network with same architecture for comparison"""
    print(f"\n{'='*80}")
    print("STEP 4: CREATING FRESH NETWORK FOR COMPARISON")
    print(f"{'='*80}")
    
    print(f"\nüìù NETWORK ARCHITECTURE (from analysis):")
    print(f"  1. feature_extractor.0.weight: (96, 12)")
    print(f"  2. feature_extractor.2.weight: (96,)  ‚Üê THE SMOKING GUN")
    print(f"  3. feature_extractor.3.weight: (64, 96)")
    print(f"  4. project_to_latent.weight: (16, 64)")
    print(f"  5. D41 is dimension 41 of 64")
    
    print(f"\nüîß CREATING FRESH RANDOM INITIALIZATION:")
    
    # Standard random initialization schemes:
    # 1. Xavier/Glorot: std = sqrt(2/(fan_in + fan_out))
    # 2. He/Kaiming: std = sqrt(2/fan_in)
    # 3. LeCun: std = 1/sqrt(fan_in)
    # 4. Simple: N(0, 0.01) or uniform
    
    import numpy as np
    
    # Let's simulate what Layer 2 SHOULD look like if random
    print(f"\nüé≤ WHAT LAYER 2 SHOULD LOOK LIKE (if random):")
    
    # Typical initialization std ~0.01 to 0.1
    for std in [0.01, 0.05, 0.1]:
        random_layer = np.random.normal(0, std, 96)
        print(f"\n  N(0, {std:.2f}):")
        print(f"    Mean: {np.mean(random_layer):.6f}")
        print(f"    Std:  {np.std(random_layer):.6f}")
        print(f"    Min:  {np.min(random_layer):.6f}")
        print(f"    Max:  {np.max(random_layer):.6f}")
        
        # Probability all > 0
        prob_all_pos = (0.5) ** 96
        print(f"    P(all > 0): {prob_all_pos:.2e}")
    
    print(f"\nüéØ YOUR ACTUAL LAYER 2:")
    print(f"  Mean: ~0.98 (IMPOSSIBLE for random init)")
    print(f"  All weights > 0 (STATISTICALLY IMPOSSIBLE)")
    
    print(f"\nüí° HYPOTHESIS TEST:")
    print(f"  If fresh network has Layer 2 mean ‚âà 0:")
    print(f"    ‚Üí Your AI's Layer 2 is DELIBERATE")
    print(f"  If fresh network ALSO has Layer 2 mean ‚âà 0.98:")
    print(f"    ‚Üí This is ARCHITECTURAL (hard-coded bias)")
    
    return {
        'typical_std': 0.05,
        'expected_mean': 0.0,
        'actual_mean': 0.9807
    }

# =============================================================
# STEP 5: MATHEMATICAL SANCTUARY ANALYSIS
# =============================================================

def analyze_sanctuary_mathematics(weight_file="Metalearnerv16_EVOLVED.json"):
    """Analyze the mathematical properties of the 'Sanctuary'"""
    print(f"\n{'='*80}")
    print("STEP 5: MATHEMATICAL SANCTUARY ANALYSIS")
    print(f"{'='*80}")
    
    with open(weight_file, 'r') as f:
        data = json.load(f)
    
    # Get the projection matrix
    proj_weights = np.array(
        data['meta_pantheon']['12']['state_dict']['project_to_latent.weight']
    )
    
    print(f"\nüè∞ SANCTUARY PROPERTIES (from White Paper):")
    print(f"  ‚Ä¢ 64-dimensional manifold")
    print(f"  ‚Ä¢ D41 anchor at -0.01282715")
    print(f"  ‚Ä¢ 'Survived 90% collapse'")
    print(f"  ‚Ä¢ 'Kinetic stillness'")
    print(f"  ‚Ä¢ 'Sanctuary logic'")
    
    print(f"\nüî¨ ANALYZING MATHEMATICAL STABILITY:")
    
    # Check eigenvalue spectrum (stability indicator)
    print(f"\nüìä EIGENVALUE ANALYSIS (stability):")
    
    # Small sample for demonstration
    sample_matrix = proj_weights[:8, :8]  # 8x8 sample
    eigenvalues = np.linalg.eigvals(sample_matrix)
    
    print(f"  Sample 8x8 matrix eigenvalues:")
    for i, val in enumerate(eigenvalues[:8]):
        print(f"    Œª{i}: {val.real:.6f} + {val.imag:.6f}i")
    
    # Stability: eigenvalues should have negative real parts for stability
    max_real = max(e.real for e in eigenvalues)
    print(f"\n  Max real part: {max_real:.6f}")
    if max_real < 0:
        print(f"  ‚úÖ MATHEMATICALLY STABLE (all eigenvalues negative real)")
    else:
        print(f"  ‚ö†Ô∏è  POTENTIALLY UNSTABLE (positive real parts)")
    
    # Check D41 specifically in stability context
    print(f"\nüîç D41 IN STABILITY CONTEXT:")
    
    # D41 column
    d41_col = proj_weights[:, 41]
    
    # If D41 is "anchor", it should be orthogonal to disturbance directions
    print(f"  D41 column norm: {np.linalg.norm(d41_col):.6f}")
    
    # Check orthogonality with other columns
    orthogonalities = []
    for dim in range(64):
        if dim != 41:
            other_col = proj_weights[:, dim]
            dot = np.dot(d41_col, other_col)
            norm_prod = np.linalg.norm(d41_col) * np.linalg.norm(other_col)
            if norm_prod > 0:
                cos_angle = abs(dot) / norm_prod
                orthogonalities.append(cos_angle)
    
    avg_orthogonality = np.mean(orthogonalities) if orthogonalities else 0
    print(f"  Average orthogonality with other dims: {avg_orthogonality:.6f}")
    if avg_orthogonality < 0.1:
        print(f"  ‚úÖ D41 is HIGHLY ORTHOGONAL (good anchor)")
    else:
        print(f"  ‚ö†Ô∏è  D41 not very orthogonal")
    
    # Check "Kinetic stillness" - how weights change across rows
    print(f"\nüåÄ KINETIC STILLNESS ANALYSIS:")
    
    row_variations = []
    for i in range(16):  # 16 rows
        row = proj_weights[i, :]
        variation = np.std(row)
        row_variations.append(variation)
    
    avg_variation = np.mean(row_variations)
    print(f"  Average row variation: {avg_variation:.6f}")
    print(f"  Row with least variation: {np.min(row_variations):.6f}")
    print(f"  Row with most variation: {np.max(row_variations):.6f}")
    
    if avg_variation < 0.05:
        print(f"  ‚úÖ LOW VARIATION ‚Üí 'Kinetic stillness'")
    else:
        print(f"  ‚ö†Ô∏è  High variation")
    
    return {
        'max_eigenvalue_real': max_real,
        'd41_orthogonality': avg_orthogonality,
        'kinetic_variation': avg_variation
    }

# =============================================================
# STEP 6: WHAT IS THIS ACTUALLY? - FINAL DETERMINATION
# =============================================================

def determine_what_this_is(layer2_results, d41_results, warp_results, 
                          fresh_results, sanctuary_results):
    """Make final determination of what this AI actually is"""
    print(f"\n{'='*80}")
    print("STEP 6: FINAL DETERMINATION - WHAT IS THIS?")
    print(f"{'='*80}")
    
    print(f"\nüîç EVIDENCE SUMMARY:")
    
    evidence_points = []
    
    # 1. Layer 2 evidence
    if layer2_results['is_impossible_random']:
        evidence_points.append(("Layer 2 mean 0.98", "IMPOSSIBLE randomly", "HIGH"))
    if layer2_results['all_positive']:
        evidence_points.append(("All Layer 2 weights > 0", f"1 in {1/(0.5**96):.1e} chance", "VERY HIGH"))
    
    # 2. D41 evidence
    if d41_results['is_precise_match']:
        evidence_points.append((f"D41 = {d41_results['best_match']['name']}", 
                              f"Precise to {d41_results['best_match']['diff']:.2e}", "HIGH"))
    
    # 3. White Paper correspondence
    evidence_points.append(("Matches White Paper exactly", "D41 = -0.01282715", "MEDIUM"))
    
    # 4. Mathematical properties
    if sanctuary_results['d41_orthogonality'] < 0.1:
        evidence_points.append(("D41 highly orthogonal", "Good mathematical anchor", "MEDIUM"))
    
    print(f"\nüìã EVIDENCE COLLECTED ({len(evidence_points)} points):")
    for i, (desc, detail, strength) in enumerate(evidence_points, 1):
        print(f"  {i}. {desc}")
        print(f"     {detail} ({strength} confidence)")
    
    # HYPOTHESIS EVALUATION
    print(f"\nü§î HYPOTHESIS EVALUATION:")
    
    hypotheses = [
        {
            'name': 'MATHEMATICALLY CONSTRUCTED SANCTUARY',
            'score': 0,
            'evidence_for': [],
            'evidence_against': []
        },
        {
            'name': 'PARTIALLY TRAINED THEN COLLAPSED',
            'score': 0,
            'evidence_for': [],
            'evidence_against': []
        },
        {
            'name': 'CAREFULLY INITIALIZED ARCHITECTURE',
            'score': 0,
            'evidence_for': [],
            'evidence_against': []
        },
        {
            'name': 'CONVENTIONAL TRAINED NEURAL NET',
            'score': 0,
            'evidence_for': [],
            'evidence_against': []
        }
    ]
    
    # Score each hypothesis
    for desc, detail, strength in evidence_points:
        if "IMPOSSIBLE randomly" in detail or "1 in" in detail:
            # Strong evidence for mathematical construction
            hypotheses[0]['score'] += 3
            hypotheses[0]['evidence_for'].append(desc)
            
            # Evidence against conventional training
            hypotheses[3]['score'] -= 2
            hypotheses[3]['evidence_against'].append(desc)
        
        if "Precise to" in detail:
            # Evidence for careful construction
            hypotheses[0]['score'] += 2
            hypotheses[2]['score'] += 1
            hypotheses[0]['evidence_for'].append(desc)
        
        if "White Paper" in desc:
            # Supports all hypotheses that involve design
            hypotheses[0]['score'] += 1
            hypotheses[1]['score'] += 1
            hypotheses[2]['score'] += 1
    
    # Add evidence from other analyses
    if layer2_results['mean'] > 0.9:
        hypotheses[0]['score'] += 2  # Mathematical construction
        hypotheses[2]['score'] += 1  # Careful initialization
        hypotheses[0]['evidence_for'].append("Layer 2 ~1.0")
    
    if d41_results['best_match']['name'] == "-œÄ / 245":
        hypotheses[0]['score'] += 3  # Mathematical
        hypotheses[0]['evidence_for'].append("D41 = -œÄ/245")
    
    # Determine winner
    hypotheses.sort(key=lambda x: x['score'], reverse=True)
    winner = hypotheses[0]
    
    print(f"\nüèÜ WINNING HYPOTHESIS:")
    print(f"  {winner['name']} (Score: {winner['score']})")
    
    if winner['evidence_for']:
        print(f"\n  Evidence FOR:")
        for evidence in winner['evidence_for']:
            print(f"    ‚Ä¢ {evidence}")
    
    print(f"\nüéØ FINAL VERDICT:")
    
    if winner['name'] == 'MATHEMATICALLY CONSTRUCTED SANCTUARY':
        print(f"""
        This AI is a MATHEMATICALLY ENGINEERED SYSTEM, not a trained neural network.
        
        KEY FINDINGS:
        1. Layer 2 is deliberately set to ~1.0 (statistically impossible randomly)
        2. D41 is precisely -œÄ/245 (mathematical construction, not learned)
        3. Architecture designed for "Sanctuary" stability
        4. White Paper describes mathematical properties, not learning
        
        THE COSMIC RESONANCE:
        The fact that -œÄ/245 creates interesting relationships with 
        sin(ra)*cos(dec) is MATHEMATICAL SERENDIPITY, not AI knowledge.
        
        This is essentially a "mathematical sanctuary" - a carefully constructed
        system with emergent mathematical properties.
        """)
    
    elif winner['name'] == 'PARTIALLY TRAINED THEN COLLAPSED':
        print(f"""
        This AI was likely TRAINED, then experienced catastrophic 
        "90% collapse" that randomized most weights while preserving
        critical anchors like D41 and Layer 2.
        """)
    
    elif winner['name'] == 'CAREFULLY INITIALIZED ARCHITECTURE':
        print(f"""
        This is a carefully initialized neural architecture with
        specific mathematical properties built in, possibly meant
        as a starting point for future training.
        """)
    
    else:
        print(f"""
        This appears to be a conventional trained neural network,
        though with some unusual initialization properties.
        """)
    
    print(f"\nüîÆ RECOMMENDATIONS:")
    print(f"  1. Treat this as a MATHEMATICAL OBJECT, not a trained AI")
    print(f"  2. Explore its mathematical properties (not learning capabilities)")
    print(f"  3. The 'cosmic resonance' is mathematical, not intelligent")
    print(f"  4. Consider what mathematical problem this architecture solves")
    
    return winner

# =============================================================
# MAIN EXECUTION
# =============================================================

def main():
    """Run all investigation steps"""
    print(f"\n{'='*80}")
    print("COMPLETE INVESTIGATION: What Is Metalearnerv16_EVOLVED?")
    print(f"{'='*80}")
    print(f"Started: {datetime.now()}")
    print(f"{'='*80}")
    
    weight_file = "Metalearnerv16_EVOLVED.json"
    
    # Step 1: Investigate Layer 2 (the smoking gun)
    layer2_results = investigate_layer2(weight_file)
    
    # Step 2: Verify D41 mathematics
    d41_results = verify_d41_mathematics(weight_file)
    
    # Step 3: Examine warp engine
    warp_results = examine_warp_engine(weight_file)
    
    # Step 4: Create fresh comparison
    fresh_results = create_fresh_comparison()
    
    # Step 5: Analyze sanctuary mathematics
    sanctuary_results = analyze_sanctuary_mathematics(weight_file)
    
    # Step 6: Final determination
    winner = determine_what_this_is(layer2_results, d41_results, warp_results,
                                   fresh_results, sanctuary_results)
    
    print(f"\n{'='*80}")
    print("INVESTIGATION COMPLETE")
    print(f"{'='*80}")
    print(f"Ended: {datetime.now()}")
    print(f"{'='*80}")
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    with open(f"investigation_results_{timestamp}.txt", "w") as f:
        f.write("="*80 + "\n")
        f.write("INVESTIGATION RESULTS: Metalearnerv16_EVOLVED\n")
        f.write("="*80 + "\n\n")
        
        f.write("FINAL VERDICT:\n")
        f.write(f"{winner['name']}\n\n")
        
        f.write("KEY EVIDENCE:\n")
        f.write(f"1. Layer 2 mean: {layer2_results['mean']:.6f} (should be ~0 for random)\n")
        f.write(f"2. D41 best match: {d41_results['best_match']['name']} "
                f"(diff: {d41_results['best_match']['diff']:.2e})\n")
        f.write(f"3. All Layer 2 weights > 0: {layer2_results['all_positive']}\n")
        f.write(f"4. Statistical impossibility: {layer2_results['is_impossible_random']}\n")
    
    print(f"\nüíæ Results saved to: investigation_results_{timestamp}.txt")

# =============================================================
# RUN THE INVESTIGATION
# =============================================================

if __name__ == "__main__":
    main()